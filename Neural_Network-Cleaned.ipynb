{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>cut_b'Good'</th>\n",
       "      <th>cut_b'Ideal'</th>\n",
       "      <th>cut_b'Premium'</th>\n",
       "      <th>cut_b'Very Good'</th>\n",
       "      <th>color_b'E'</th>\n",
       "      <th>color_b'F'</th>\n",
       "      <th>...</th>\n",
       "      <th>color_b'H'</th>\n",
       "      <th>color_b'I'</th>\n",
       "      <th>color_b'J'</th>\n",
       "      <th>clarity_b'IF'</th>\n",
       "      <th>clarity_b'SI1'</th>\n",
       "      <th>clarity_b'SI2'</th>\n",
       "      <th>clarity_b'VS1'</th>\n",
       "      <th>clarity_b'VS2'</th>\n",
       "      <th>clarity_b'VVS1'</th>\n",
       "      <th>clarity_b'VVS2'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  depth  table  price  cut_b'Good'  cut_b'Ideal'  cut_b'Premium'  \\\n",
       "0   0.23   61.5   55.0  326.0            0             1               0   \n",
       "1   0.21   59.8   61.0  326.0            0             0               1   \n",
       "2   0.23   56.9   65.0  327.0            1             0               0   \n",
       "3   0.29   62.4   58.0  334.0            0             0               1   \n",
       "4   0.31   63.3   58.0  335.0            1             0               0   \n",
       "\n",
       "   cut_b'Very Good'  color_b'E'  color_b'F'  ...  color_b'H'  color_b'I'  \\\n",
       "0                 0           1           0  ...           0           0   \n",
       "1                 0           1           0  ...           0           0   \n",
       "2                 0           1           0  ...           0           0   \n",
       "3                 0           0           0  ...           0           1   \n",
       "4                 0           0           0  ...           0           0   \n",
       "\n",
       "   color_b'J'  clarity_b'IF'  clarity_b'SI1'  clarity_b'SI2'  clarity_b'VS1'  \\\n",
       "0           0              0               0               1               0   \n",
       "1           0              0               1               0               0   \n",
       "2           0              0               0               0               1   \n",
       "3           0              0               0               0               0   \n",
       "4           1              0               0               1               0   \n",
       "\n",
       "   clarity_b'VS2'  clarity_b'VVS1'  clarity_b'VVS2'  \n",
       "0               0                0                0  \n",
       "1               0                0                0  \n",
       "2               0                0                0  \n",
       "3               1                0                0  \n",
       "4               0                0                0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "diamond_df = pd.read_csv(\"Resources/cleaned.csv\")\n",
    "diamond_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "#try to predict carat\n",
    "\n",
    "y = diamond_df['carat']\n",
    "\n",
    "\n",
    "X = diamond_df.drop('carat', axis=1)\n",
    "\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test,y_train,y_test =train_test_split(X,y, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035705302343445"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data could be potentially scaled again but there was no need for the scope of the project\n",
    "# Create a StandardScaler instances\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "#X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "#X_train_scaled = X_scaler.transform(X_train)\n",
    "#X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15)                165       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 546\n",
      "Trainable params: 546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "\n",
    "diamonds_nn= len(X_train.columns)\n",
    "hidden_node_layer = 10\n",
    "second_hidden_node_layer = 10\n",
    "third_hidden_layer= 15\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_node_layer, input_dim=diamonds_nn, activation='relu'))\n",
    "#additional layer\n",
    "nn.add(tf.keras.layers.Dense(units=third_hidden_layer, activation='relu'))\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=second_hidden_node_layer, activation='relu'))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29817"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn.compile(optimizer='adam',loss='mean_squared_error', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "932/932 [==============================] - 3s 2ms/step - loss: 6947.5894 - mean_squared_error: 6947.5894\n",
      "Epoch 2/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 1.0210 - mean_squared_error: 1.0210\n",
      "Epoch 3/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.8855 - mean_squared_error: 0.8855\n",
      "Epoch 4/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.7967 - mean_squared_error: 0.7967\n",
      "Epoch 5/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.6953 - mean_squared_error: 0.6953\n",
      "Epoch 6/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.5710 - mean_squared_error: 0.5710\n",
      "Epoch 7/20\n",
      "932/932 [==============================] - 1s 2ms/step - loss: 0.4390 - mean_squared_error: 0.4390\n",
      "Epoch 8/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.3183 - mean_squared_error: 0.3183\n",
      "Epoch 9/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.2173 - mean_squared_error: 0.2173\n",
      "Epoch 10/20\n",
      "932/932 [==============================] - 1s 2ms/step - loss: 0.1499 - mean_squared_error: 0.1499\n",
      "Epoch 11/20\n",
      "932/932 [==============================] - 1s 2ms/step - loss: 0.1180 - mean_squared_error: 0.1180\n",
      "Epoch 12/20\n",
      "932/932 [==============================] - 1s 2ms/step - loss: 0.0928 - mean_squared_error: 0.0928\n",
      "Epoch 13/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 14/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.0845 - mean_squared_error: 0.0845\n",
      "Epoch 15/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.0743 - mean_squared_error: 0.0743\n",
      "Epoch 16/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.0817 - mean_squared_error: 0.0817\n",
      "Epoch 17/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.1987 - mean_squared_error: 0.1987\n",
      "Epoch 18/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.2307 - mean_squared_error: 0.2307\n",
      "Epoch 19/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.2307 - mean_squared_error: 0.2307\n",
      "Epoch 20/20\n",
      "932/932 [==============================] - 1s 2ms/step - loss: 0.2307 - mean_squared_error: 0.2307\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = nn.fit(X_train, y_train, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0005032059339393768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r2_score(y_test, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 - 0s - loss: 0.2267 - mean_squared_error: 0.2267 - 470ms/epoch - 2ms/step\n",
      "Loss: 0.22672587633132935, Mean Squared Error: 0.22672587633132935\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Mean Squared Error: {model_accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = diamond_df['depth']\n",
    "y\n",
    "\n",
    "X = diamond_df.drop('depth', axis=1)\n",
    "X\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test,y_train,y_test =train_test_split(X,y, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data could be potentially scaled again but there was no need for the scope of the project\n",
    "# Create a StandardScaler instances\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "#X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "#X_train_scaled = X_scaler.transform(X_train)\n",
    "#X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 15)                165       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 546\n",
      "Trainable params: 546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "\n",
    "diamonds_nn= len(X_train.columns)\n",
    "hidden_node_layer = 10\n",
    "second_hidden_node_layer = 10\n",
    "third_hidden_layer= 15\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_node_layer, input_dim=diamonds_nn, activation='relu'))\n",
    "#additional layer\n",
    "nn.add(tf.keras.layers.Dense(units=third_hidden_layer, activation='relu'))\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=second_hidden_node_layer, activation='relu'))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn.compile(optimizer='adam',loss='mean_squared_error', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 1376.3770 - mean_squared_error: 1376.3770\n",
      "Epoch 2/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.3560 - mean_squared_error: 10.3560\n",
      "Epoch 3/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.1755 - mean_squared_error: 10.1755\n",
      "Epoch 4/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.0698 - mean_squared_error: 10.0698\n",
      "Epoch 5/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.4040 - mean_squared_error: 10.4040\n",
      "Epoch 6/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.0215 - mean_squared_error: 10.0215\n",
      "Epoch 7/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.7480 - mean_squared_error: 10.7480\n",
      "Epoch 8/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.0274 - mean_squared_error: 10.0274\n",
      "Epoch 9/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.1883 - mean_squared_error: 10.1883\n",
      "Epoch 10/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.8156 - mean_squared_error: 10.8156\n",
      "Epoch 11/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 9.8422 - mean_squared_error: 9.8422\n",
      "Epoch 12/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 11.1442 - mean_squared_error: 11.1442\n",
      "Epoch 13/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 9.9087 - mean_squared_error: 9.9087\n",
      "Epoch 14/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 9.6484 - mean_squared_error: 9.6484\n",
      "Epoch 15/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 9.3728 - mean_squared_error: 9.3728\n",
      "Epoch 16/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 11.1139 - mean_squared_error: 11.1139\n",
      "Epoch 17/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 9.1815 - mean_squared_error: 9.1815\n",
      "Epoch 18/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 9.6184 - mean_squared_error: 9.6184\n",
      "Epoch 19/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 9.3433 - mean_squared_error: 9.3433\n",
      "Epoch 20/20\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 10.1912 - mean_squared_error: 10.1912\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = nn.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = nn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.37236328827171"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 - 0s - loss: 7.5805 - mean_squared_error: 7.5805 - 462ms/epoch - 1ms/step\n",
      "Loss: 7.580453872680664, Mean Squared Error: 7.580453872680664\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Mean Squared Error: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
